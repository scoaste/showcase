{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movielens predictions using pyspark and mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define imports and some initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the all important Spark context\n",
    "conf = (SparkConf()\n",
    "        .setMaster('yarn-client')\n",
    "        .setAppName('Movielens Prediction Model')\n",
    "       )\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set to True to redownload the data and retrain the prediction model\n",
    "retrain_model = True\n",
    "\n",
    "# data source URLs\n",
    "dataset_url = 'http://files.grouplens.org/datasets/movielens'\n",
    "small_dataset_url = dataset_url + '/ml-latest-small.zip'\n",
    "complete_dataset_url = dataset_url + '/ml-latest.zip'\n",
    "\n",
    "# data local file system destination names\n",
    "datasets_path = '/home/ste328/pyspark/movielens'\n",
    "small_dataset_path = datasets_path +  '/ml-latest-small'\n",
    "complete_dataset_path = datasets_path + '/ml-latest'\n",
    "small_dataset_zip = small_dataset_path +  '.zip'\n",
    "complete_dataset_zip = complete_dataset_path + '.zip'\n",
    "\n",
    "# data HDFS paths\n",
    "datasets_hdfs_path = '/user/ste328/spark/movielens'\n",
    "\n",
    "# HDFS client\n",
    "client = InsecureClient('http://devctlvhadapp02.iteclientsys.local:50070', user='ste328')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the latest movie data and write it to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ste328/pyspark/movielens/ml-latest-small.zip\n",
      "/home/ste328/pyspark/movielens/ml-latest.zip\n",
      "/user/ste328/spark/movielens/ml-latest-small\n",
      "/user/ste328/spark/movielens/ml-latest\n"
     ]
    }
   ],
   "source": [
    "if(retrain_model):\n",
    "    # Retrieve the data archives to local storage\n",
    "    (small_dataset_filename, small_dataset_headers) = urllib.urlretrieve(small_dataset_url, small_dataset_zip)\n",
    "    (complete_dataset_filename, complete_dataset_headers) = urllib.urlretrieve(complete_dataset_url, complete_dataset_zip)\n",
    "    print small_dataset_filename\n",
    "    print complete_dataset_filename\n",
    "    # Unzip the files\n",
    "    with zipfile.ZipFile(small_dataset_filename, 'r') as z:\n",
    "        z.extractall(datasets_path)\n",
    "    with zipfile.ZipFile(complete_dataset_filename, 'r') as z:\n",
    "        z.extractall(datasets_path)\n",
    "    # Copy the unzipped files to HDFS\n",
    "    small_dataset_hdfs_path = client.upload(datasets_hdfs_path, small_dataset_path, overwrite=True)\n",
    "    complete_dataset_hdfs_path = client.upload(datasets_hdfs_path, complete_dataset_path, overwrite=True)\n",
    "else:\n",
    "    small_dataset_hdfs_path = '/user/ste328/spark/movielens/ml-latest-small'\n",
    "    complete_dataset_hdfs_path = '/user/ste328/spark/movielens/ml-latest'\n",
    "\n",
    "print small_dataset_hdfs_path\n",
    "print complete_dataset_hdfs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16, 4.0)]\n"
     ]
    }
   ],
   "source": [
    "if(retrain_model):\n",
    "    # ('userId', 'movieId', 'rating', 'timestamp')\n",
    "    small_ratings_raw_data = sc.textFile(small_dataset_hdfs_path + '/ratings.csv')\n",
    "    small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]\n",
    "    small_ratings_data = small_ratings_raw_data\\\n",
    "        .filter(lambda line: line != small_ratings_raw_data_header)\\\n",
    "        .map(lambda line: line.split(\",\"))\\\n",
    "        .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2])))\\\n",
    "        .cache().coalesce(1000, shuffle=True)\n",
    "    print small_ratings_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the small ratings data into training, validation, & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 24, 1.5)]\n",
      "[(1, 47, 4.0)]\n",
      "[(1, 16, 4.0)]\n",
      "[(1, 47)]\n",
      "[(1, 16)]\n"
     ]
    }
   ],
   "source": [
    "if(retrain_model):\n",
    "    # training 60%, validation 20%, test 20%\n",
    "    (training_RDD, validation_RDD, test_RDD) = small_ratings_data.randomSplit([6, 2, 2], seed=0L)\n",
    "    # remove 'rating' for validation and test predictions\n",
    "    validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "    test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "    print training_RDD.take(1)\n",
    "    print validation_RDD.take(1)\n",
    "    print test_RDD.take(1)\n",
    "    print validation_for_predict_RDD.take(1)\n",
    "    print test_for_predict_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the predictions model using the Alternating Least Squares (ALS) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For regularization parameter 0.1 and rank 2 the RMSE is 0.91811132541\n",
      "For regularization parameter 0.1 and rank 3 the RMSE is 0.915280020988\n",
      "For regularization parameter 0.1 and rank 4 the RMSE is 0.922031953997\n",
      "For regularization parameter 0.1 and rank 5 the RMSE is 0.927868038721\n",
      "For regularization parameter 0.15 and rank 2 the RMSE is 0.913719406416\n",
      "For regularization parameter 0.15 and rank 3 the RMSE is 0.904759451544\n",
      "For regularization parameter 0.15 and rank 4 the RMSE is 0.90579008604\n",
      "For regularization parameter 0.15 and rank 5 the RMSE is 0.907780847352\n",
      "For regularization parameter 0.2 and rank 2 the RMSE is 0.915021152829\n",
      "For regularization parameter 0.2 and rank 3 the RMSE is 0.905806135304\n",
      "For regularization parameter 0.2 and rank 4 the RMSE is 0.905285841847\n",
      "For regularization parameter 0.2 and rank 5 the RMSE is 0.906176554803\n",
      "For regularization parameter 0.25 and rank 2 the RMSE is 0.921276046909\n",
      "For regularization parameter 0.25 and rank 3 the RMSE is 0.914804680867\n",
      "For regularization parameter 0.25 and rank 4 the RMSE is 0.913754074497\n",
      "For regularization parameter 0.25 and rank 5 the RMSE is 0.914774392693\n",
      "The best model was trained with regularization parameter 0.15 and rank 3\n"
     ]
    }
   ],
   "source": [
    "if(retrain_model):\n",
    "    seed = 5L\n",
    "    iterations = 15\n",
    "    regularization_parameters = np.linspace(0.1, 0.25, 4, dtype=float)\n",
    "    ranks = np.linspace(2, 5, 4, dtype=int)\n",
    "    min_error = float('inf') #infinity\n",
    "    best_rank = -1\n",
    "    best_regularization_parameter = -1\n",
    "\n",
    "    for regularization_parameter in regularization_parameters:\n",
    "        for rank in ranks:\n",
    "            model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations, lambda_=regularization_parameter)\n",
    "            predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            rates_and_preds = validation_RDD.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "            print 'For regularization parameter %s and rank %s the RMSE is %s' % (regularization_parameter, rank, error)\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization_parameter = regularization_parameter\n",
    "\n",
    "    print 'The best model was trained with regularization parameter %s and rank %s' % (best_regularization_parameter, best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the best ranked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.901231695512\n"
     ]
    }
   ],
   "source": [
    "if(retrain_model):\n",
    "    model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations, lambda_=best_regularization_parameter)\n",
    "    predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = test_RDD.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "    print 'For testing data the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset variables and uncache the small dataset to conserve memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(retrain_model):\n",
    "    small_ratings_data.unpersist()\n",
    "    %reset_selective -f small_ratings_raw_data\n",
    "    %reset_selective -f small_ratings_raw_data_header\n",
    "    %reset_selective -f small_ratings_data\n",
    "    %reset_selective -f training_RDD\n",
    "    %reset_selective -f validation_RDD\n",
    "    %reset_selective -f test_RDD\n",
    "    %reset_selective -f validation_for_predict_RDD\n",
    "    %reset_selective -f test_for_predict_RDD\n",
    "    %reset_selective -f model\n",
    "    %reset_selective -f predictions\n",
    "    %reset_selective -f rates_and_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the  complete ratings dataset\n",
    "#### (NOTE: make sure Spark is running in YARN cluster or client mode (using Python 2.7) or likely Java will run out of heap.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22884377 recommendations in the complete dataset\n",
      "[(1, 169, 2.5)]\n"
     ]
    }
   ],
   "source": [
    "# ('userId', 'movieId', 'rating', 'timestamp')\n",
    "complete_ratings_raw_data = sc.textFile(complete_dataset_hdfs_path + '/ratings.csv')\n",
    "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
    "# Create more partitions for this RDD to save on memory usage.\n",
    "complete_ratings_data_RDD = complete_ratings_raw_data\\\n",
    "    .filter(lambda line: line != complete_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\"))\\\n",
    "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))).cache().coalesce(1000, shuffle=True)\n",
    "\n",
    "print \"There are %s recommendations in the complete dataset\" % (complete_ratings_data_RDD.count())\n",
    "print complete_ratings_data_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read in the complete movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34208 movies in the complete dataset\n",
      "[(1, u'Toy Story (1995)')]\n"
     ]
    }
   ],
   "source": [
    "# ('movieId', 'title', 'genres')\n",
    "complete_movies_raw_data = sc.textFile(complete_dataset_hdfs_path + '/movies.csv')\n",
    "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "# Create more partitions for this RDD to save on memory usage.\n",
    "complete_movies_data_RDD = complete_movies_raw_data\\\n",
    "    .filter(lambda line: line != complete_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\"))\\\n",
    "    .map(lambda tokens: (int(tokens[0]), tokens[1])).cache().coalesce(1000, shuffle=True)\n",
    "\n",
    "print \"There are %s movies in the complete dataset\" % (complete_movies_data_RDD.count())\n",
    "print complete_movies_data_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and average the ratings and join them to the movies (for prediction selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts_and_averages(movieID_and_ratings_tuple):\n",
    "    num_ratings = len(movieID_and_ratings_tuple[1])\n",
    "    # (movieId, (count, average))\n",
    "    return movieID_and_ratings_tuple[0], (num_ratings, float(sum(movieID_and_ratings_tuple[1]))/num_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32770, (156, 3.6538461538461537))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('userId', 'movieId', 'rating', 'timestamp')\n",
    "movie_ID_with_ratings_RDD = complete_ratings_data_RDD.map(lambda x: (x[1], x[2])).groupByKey()\n",
    "movie_ID_with_ratings_aggregates_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_ID_with_ratings_aggregates_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(106498, (u'\"Magic Voyage of Sindbad', (7, 3.2142857142857144)))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_movies_with_aggregates_RDD = complete_movies_data_RDD.join(movie_ID_with_ratings_aggregates_RDD)\n",
    "complete_movies_with_aggregates_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final commender model using the complete ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retrain_model = True # why is this variable lost?\n",
    "if(retrain_model):\n",
    "    training_RDD, test_RDD = complete_ratings_data_RDD.randomSplit([7, 3], seed=0L)\n",
    "    complete_model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
    "                               lambda_=best_regularization_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and reload the recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import MatrixFactorizationModel as mfm\n",
    "\n",
    "model_path = '/user/ste328/spark/movielens/models/als'\n",
    "\n",
    "if(retrain_model):\n",
    "    client.delete(model_path, True)\n",
    "    complete_model.save(sc, model_path)\n",
    "    %reset_selective -f complete_model\n",
    "\n",
    "complete_model = mfm.load(sc, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sample recommendations for a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get a tuple of user ID and movie ID for movies not rated by this sample user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(470, 133379)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ID = 470\n",
    "# ('userId', 'movieId', 'rating', 'timestamp')\n",
    "user_unrated_movies_RDD = complete_ratings_data_RDD\\\n",
    "    .filter(lambda x: x[0] != user_ID)\\\n",
    "    .map(lambda x: (user_ID, x[1]))\\\n",
    "    .distinct()\n",
    "user_unrated_movies_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=470, product=80928, rating=4.034831279303006)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movie_predictions_RDD = complete_model.predictAll(user_unrated_movies_RDD)\n",
    "user_movie_predictions_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Join the movie predictions with their titles, ratings counts, and ratings average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(98304, (3.996685905270772, (u'So Big! (1932)', (3, 2.8333333333333335))))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_predictions_RDD = user_movie_predictions_RDD\\\n",
    "    .map(lambda x: (x.product, x.rating))\\\n",
    "    .join(complete_movies_with_aggregates_RDD)\n",
    "movie_predictions_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Flatten out the nested tuples, take only movie predictions with more than 25 ratings and where the predicted rating is > the average rating, and take the 10 best average rated movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 5.014100407391223, u'\"Shawshank Redemption', 77887, 4.441710426643727)\n",
      "(858, 4.93947100819213, u'\"Godfather', 49846, 4.35363920876299)\n",
      "(50, 4.8723661201458, u'\"Usual Suspects', 53195, 4.318986746874706)\n",
      "(527, 4.949440898634176, u\"Schindler's List (1993)\", 59857, 4.2909517683813085)\n",
      "(142115, 5.327760888663839, u'The Blue Planet (2001)', 30, 4.283333333333333)\n",
      "(140737, 4.8364313591299215, u'The Lost Room (2006)', 73, 4.280821917808219)\n",
      "(1221, 4.822264059094632, u'\"Godfather: Part II', 32247, 4.268877725059696)\n",
      "(2019, 4.851535806927216, u'Seven Samurai (Shichinin no samurai) (1954)', 12753, 4.262134399749079)\n",
      "(904, 4.903571401818233, u'Rear Window (1954)', 19422, 4.246987951807229)\n",
      "(1193, 4.8380592369765445, u\"One Flew Over the Cuckoo's Nest (1975)\", 35832, 4.24245088189328)\n"
     ]
    }
   ],
   "source": [
    "# (0=movie prediction, 1=rating prediction, 2=title, 3=count, 4=average)\n",
    "movie_predictions_flat_RDD = movie_predictions_RDD\\\n",
    "    .map(lambda x: (x[0], x[1][0], x[1][1][0], x[1][1][1][0], x[1][1][1][1]))\\\n",
    "    .filter(lambda x: x[3] >= 25 and x[1] > x[4])\\\n",
    "    .takeOrdered(10, key = lambda x: -x[4])\n",
    "\n",
    "print '\\n'.join(map(str,movie_predictions_flat_RDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
